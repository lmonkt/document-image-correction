> 在[上一篇文章](技术分享-文档图像去倾斜与旋转校正方法.md)中，已确定基于 FFT 频谱分析的 jdeskew-exp 方法是去倾斜的最优选择（推荐参数：`amax=45, V=2048, W=304, D=0.55`）。然而在实际部署中，该算法单张处理耗时约 **220ms**，而方向分类 ONNX 模型可轻松迁移到 GPU/NPU 以获得加速，**去倾斜算法的纯 CPU 实现反而成了整个文档矫正链路的速度瓶颈**。
>
> 本文记录对去倾斜算法从性能分析到 GPU/NPU 加速的完整优化过程，涵盖：热点定位、CPU 侧优化（NumPy 向量化 + Numba JIT）、NVIDIA GPU 加速（CuPy）、以及 Ascend NPU 适配（torch_npu + asnumpy 方案对比），最终实现 **9x（GPU）/ 4x（NPU）** 的加速。

---

## 一、性能分析：时间花在哪里？

### 1. 分析前提与方法

在上一篇文章中，去倾斜方法的性能测试使用了多进程并行框架。多进程会引入调度和进程间通信噪声，不利于定位算法本体的耗时分布。因此，第一步是将评估脚本改为**单进程串行执行**，并在算法关键阶段插入计时点。

用于性能分析的工具：

- **`py-spy`**：采样式性能分析器，可生成火焰图，直观展示调用栈的时间分布，无需修改代码。
- **`line_profiler`**（通过 `kernprof` 驱动）：逐行级别的耗时统计，可精确到每一行 Python 代码的执行次数和累计耗时。

安装命令：

```bash
pip install py-spy line_profiler
```

📄 **性能分析脚本请参阅**：[for_eval_deskew/jdeskew_exp_single_process_profile.py](for_eval_deskew/jdeskew_exp_single_process_profile.py)

该脚本在算法内部插入了分阶段计时点，覆盖以下阶段：

```
read → resize → gray → optimal_square → adaptive_threshold → fft_and_magnitude → radial_projection → decision
```

### 2. 分析结果

测试条件：`assets` 目录下 10 张图像，每图重复 3 次，预热 2 次。

📄 **完整分析报告请参阅**：[for_eval_deskew/JDESKEW_EXP_PERF_ANALYSIS.md](for_eval_deskew/JDESKEW_EXP_PERF_ANALYSIS.md)

#### 端到端耗时

| 指标       | 耗时             |
| ---------- | ---------------- |
| 端到端平均 | `228.31 ms / 张` |
| 算法平均   | `219.54 ms / 张` |
| 读图平均   | `7.66 ms / 张`   |

瓶颈在算法本体，I/O 占比很小。

#### 算法内部耗时占比

以算法总耗时 `219.54 ms` 为基准：

| 阶段                                   | 平均耗时    | 占比       |
| -------------------------------------- | ----------- | ---------- |
| **`fft_and_magnitude`**                | `149.37 ms` | **68.04%** |
| **`radial_projection`**                | `47.38 ms`  | **21.58%** |
| `adaptive_threshold`                   | `16.51 ms`  | 7.52%      |
| 其余（resize / gray / pad / decision） | ≈ `6.28 ms` | 2.86%      |

#### 热点代码

**第一热点——FFT + 频移 + 取模（68%）：**

```python
dft = np.fft.fft2(opt_gray)
shifted_dft = np.fft.fftshift(dft)
magnitude = np.abs(shifted_dft)
```

其中 `np.fft.fft2` 占据绝对主导（约 71%），是整个算法最慢的单一操作。

**第二热点——径向投影循环（22%）：**

```python
tr = np.linspace(-1 * amax, amax, int(amax * num * 2)) / 180 * np.pi
li_init = np.zeros_like(tr)
li_correct = np.zeros_like(tr)

for i, t in enumerate(tr):
    x = np.arange(0, r)
    y = c + np.int32(x * np.cos(t))
    x_proj = c + np.int32(-1 * x * np.sin(t))
    valid = (y >= 0) & (y < m.shape[0]) & (x_proj >= 0) & (x_proj < m.shape[1])
    vals = m[y[valid], x_proj[valid]]
    if vals.size == 0:
        continue
    li_init[i] = np.sum(vals)
    li_correct[i] = np.sum(vals[W:]) if W > 0 else li_init[i]
```

该循环对 1800 个角度（`amax=45, num=20`）逐一执行索引采样与求和，Python 层面的循环开销显著。

### 3. 优化策略

根据以上分析，优化路线明确：

1. **径向投影（22%）**：涉及 Python `for` 循环，可通过向量化或 JIT 编译在 CPU 侧先行加速。
2. **FFT（68%）**：`np.fft.fft2` 已是高度优化的 C 实现，CPU 侧提升空间有限，需要迁移到 GPU/NPU 才能获得质变。

因此，优化分两步走：先在 CPU 侧优化径向投影以验证可行性，再将 FFT 和径向投影整体迁移到加速硬件。

---

## 二、CPU 侧优化：NumPy 向量化 + Numba JIT

### 1. NumPy 向量化（cpu_numpy_vec）

核心思路：将"逐角度 Python 循环 + 逐点索引"改为**批量广播矩阵运算**，一次性构造所有角度的二维索引矩阵，用 NumPy 内建的向量化操作完成 gather 和 reduce，避免 Python 解释器层面的循环开销。

改动前（原始版本循环体）：

```python
for i, t in enumerate(tr):
    x = np.arange(0, r)
    y = c + np.int32(x * np.cos(t))
    x_proj = c + np.int32(-1 * x * np.sin(t))
    valid = (y >= 0) & (y < m.shape[0]) & (x_proj >= 0) & (x_proj < m.shape[1])
    vals = m[y[valid], x_proj[valid]]
    li_init[i] = np.sum(vals)
    li_correct[i] = np.sum(vals[W:]) if W > 0 else li_init[i]
```

改动后（向量化版本）：

```python
x = np.arange(r, dtype=np.float64)
cos_t = np.cos(tr)[:, None]       # shape: (num_angles, 1)
sin_t = np.sin(tr)[:, None]

y = c + np.int32(x[None, :] * cos_t)        # shape: (num_angles, r)
x_proj = c + np.int32(-x[None, :] * sin_t)

valid = (y >= 0) & (y < m.shape[0]) & (x_proj >= 0) & (x_proj < m.shape[1])
gathered = np.zeros_like(y, dtype=np.float64)
gathered[valid] = m[y[valid], x_proj[valid]]

li_init = np.sum(gathered, axis=1)
li_correct = np.sum(gathered[:, W:], axis=1)
```

通过广播机制，`cos_t` 与 `x` 直接生成 `(num_angles, r)` 的索引矩阵，所有角度的采样和求和在一次 NumPy 调用中完成。

### 2. Numba JIT 编译（cpu_numba_jit）

Numba 是一个 Python JIT（Just-In-Time，即时编译）编译器，能够将带有 `@njit` 装饰器的 Python 函数编译为**机器码**，执行速度接近 C/Fortran。与 NumPy 向量化的"改写循环为矩阵运算"不同，Numba 的优势在于**保留原始 `for` 循环结构的同时**，消除 Python 解释器的逐行执行开销。

核心改动——对径向投影循环添加 `@njit` 装饰器：

```python
from numba import njit

@njit(cache=True, fastmath=True)
def _radial_projection_numba_core(m, tr, W):
    r = m.shape[0] // 2
    c = r
    li_init = np.zeros(tr.shape[0], dtype=np.float64)
    li_correct = np.zeros(tr.shape[0], dtype=np.float64)

    for i in range(tr.shape[0]):
        t = tr[i]
        s0 = 0.0
        s1 = 0.0
        for j in range(r):
            y = c + int(j * np.cos(t))
            x_proj = c + int(-j * np.sin(t))
            if y < 0 or y >= m.shape[0] or x_proj < 0 or x_proj >= m.shape[1]:
                continue
            v = m[y, x_proj]
            s0 += v
            if j >= W:
                s1 += v
        li_init[i] = s0
        li_correct[i] = s1 if W > 0 else s0
    return li_init, li_correct
```

其中：

- `cache=True`：编译结果缓存到磁盘，避免每次启动的编译开销（首次运行会有约 1-2 秒的编译耗时）。
- `fastmath=True`：允许对浮点运算做激进优化（如重排加法顺序），换取更高的执行速度。

📄 **完整实现代码请参阅**：

- 向量化版本：[speed_optimization_workbench/methods/method_cpu_numpy_vec.py](speed_optimization_workbench/methods/method_cpu_numpy_vec.py)
- Numba JIT 版本：[speed_optimization_workbench/methods/method_cpu_numba_jit.py](speed_optimization_workbench/methods/method_cpu_numba_jit.py)
- 原始版本（基线对照）：[speed_optimization_workbench/methods/method_original.py](speed_optimization_workbench/methods/method_original.py)

### 3. CPU 优化测试结果

测试条件：`assets` 目录前 15 张图像，每图重复 3 次，预热 2 次。

#### 端到端耗时对比

| 方法          | 平均耗时    | 加速比    |
| ------------- | ----------- | --------- |
| original      | `218.03 ms` | 1.00x     |
| cpu_numpy_vec | `207.20 ms` | **1.05x** |
| cpu_numba_jit | `183.70 ms` | **1.19x** |

#### 分阶段耗时对比

| 方法          | FFT 平均    | 径向投影平均   |
| ------------- | ----------- | -------------- |
| original      | `167.77 ms` | `46.26 ms`     |
| cpu_numpy_vec | `170.25 ms` | **`35.78 ms`** |
| cpu_numba_jit | `169.25 ms` | **`12.15 ms`** |

### 4. CPU 优化小结

两种优化对径向投影阶段均有明显加速：

- **向量化**将径向投影耗时从 `46.26 ms` 降到 `35.78 ms`（**↓23%**）
- **Numba JIT** 将径向投影耗时从 `46.26 ms` 降到 `12.15 ms`（**↓74%**）

但由于径向投影仅占算法总耗时的 22%，而占 68% 的 FFT 阶段未做任何改动，因此端到端的提升有限（最高 1.19x）。**CPU 侧优化已触及天花板，后续需要迁移到加速硬件。**

📄 **CPU 优化详细记录请参阅**：[speed_optimization_workbench/docs/STEP1_CPU_OPTIMIZATION.md](speed_optimization_workbench/docs/STEP1_CPU_OPTIMIZATION.md)

---

## 三、NVIDIA GPU 加速：CuPy 方案

### 1. 方案选型

将算法迁移到 GPU 有两条路线：

| 方案           | 优点                                | 缺点                               |
| -------------- | ----------------------------------- | ---------------------------------- |
| PyTorch tensor | 项目中已有 torch 依赖               | 安装体积大；FFT 接口不如 CuPy 直观 |
| **CuPy**       | API 与 NumPy 高度一致，迁移成本极低 | 新增一个依赖                       |

最终选择 **CuPy**，核心原因是其 API 设计几乎与 NumPy 一一对应，迁移工作量最小——主要就是将 `np.` 替换为 `cp.`。

### 2. 实现说明

改动集中在两处：

**FFT 阶段（主要加速来源）：**

```python
import cupy as cp

# 原始 NumPy 版本
dft = np.fft.fft2(opt_gray)
shifted_dft = np.fft.fftshift(dft)
magnitude = np.abs(shifted_dft)

# CuPy GPU 版本
g = cp.asarray(opt_gray, dtype=cp.float32)  # CPU → GPU
dft = cp.fft.fft2(g)
shifted = cp.fft.fftshift(dft)
magnitude = cp.abs(shifted)
```

**径向投影阶段（同样向量化到 GPU）：**

```python
# 原始版本：Python for 循环
for i, t in enumerate(tr):
    ...

# CuPy 版本：GPU 上的广播 + gather + reduce
x = cp.arange(r, dtype=cp.float32)
cos_t = cp.cos(tr)[:, None]
sin_t = cp.sin(tr)[:, None]

y = c + (x[None, :] * cos_t).astype(cp.int32)
x_proj = c + (-x[None, :] * sin_t).astype(cp.int32)

valid = (y >= 0) & (y < m.shape[0]) & (x_proj >= 0) & (x_proj < m.shape[1])
gathered = cp.zeros_like(y, dtype=cp.float32)
gathered[valid] = m[y[valid], x_proj[valid]]

li_init = cp.sum(gathered, axis=1)
li_correct = cp.sum(gathered[:, W:], axis=1)
```

整体流程设计为：CPU 完成前处理（resize / 灰度 / 自适应二值化）→ 一次性上传到 GPU → 在 GPU 上完成 FFT + 径向投影 → 仅回传标量角度值。避免 CPU ↔ GPU 间的反复数据拷贝。

📄 **完整实现代码请参阅**：[speed_optimization_workbench/methods/method_cupy.py](speed_optimization_workbench/methods/method_cupy.py)

### 3. 测试环境

| 项目         | 配置                                                   |
| ------------ | ------------------------------------------------------ |
| GPU          | NVIDIA GeForce RTX 3090                                |
| Driver       | 591.44                                                 |
| CUDA（nvcc） | 12.9                                                   |
| Python       | 3.10.12 (venv)                                         |
| CuPy         | `cupy-cuda12x`（安装命令：`pip install cupy-cuda12x`） |

### 4. GPU 加速测试结果

测试条件：`assets` 目录前 15 张图像，每图重复 3 次，预热 2 次。

#### 端到端耗时对比

| 方法           | 平均耗时       | 加速比    |
| -------------- | -------------- | --------- |
| original       | `224.35 ms`    | 1.00x     |
| cpu_numpy_vec  | `212.41 ms`    | 1.06x     |
| cpu_numba_jit  | `179.37 ms`    | 1.25x     |
| **cupy (GPU)** | **`24.59 ms`** | **9.12x** |

#### 分阶段耗时对比

| 方法           | FFT 平均       | 径向投影平均  |
| -------------- | -------------- | ------------- |
| original       | `172.20 ms`    | `48.29 ms`    |
| cpu_numba_jit  | `166.90 ms`    | `11.30 ms`    |
| **cupy (GPU)** | **`21.19 ms`** | **`1.95 ms`** |

FFT 阶段从 `172.20 ms` 降到 `21.19 ms`（**↓87.7%**），符合 FFT 天然适合 GPU 并行的预期。径向投影从 `48.29 ms` 降到 `1.95 ms`（**↓96.0%**），GPU 广播 + reduce 的效果同样突出。

### 5. 安装与使用

安装：

```bash
pip install cupy-cuda12x
```

使用示例：

```python
from speed_optimization_workbench.methods.method_cupy import get_angle

import cv2
image = cv2.imread("input.jpg")
angle = get_angle(image, amax=45.0, V=2048, W=304, D=0.55)
print(f"倾斜角度: {angle:.4f}°")
```

命令行用法：

```bash
python -m speed_optimization_workbench.methods.method_cupy input.jpg
```

📄 **GPU 优化详细记录请参阅**：[speed_optimization_workbench/docs/STEP2_CUPY_OPTIMIZATION.md](speed_optimization_workbench/docs/STEP2_CUPY_OPTIMIZATION.md)

---

## 四、Ascend NPU 适配：torch_npu 与 asnumpy

CuPy 方案依赖 NVIDIA CUDA，只能在 NVIDIA GPU 上运行。项目需求要求在华为 Ascend NPU（如 910B）上同样实现加速，因此需要额外的 NPU 适配工作。

### 1. 方案概览

经调研，NPU 侧有两条可选路线：

| 方案          | 说明                          | 优点                           | 缺点                             |
| ------------- | ----------------------------- | ------------------------------ | -------------------------------- |
| **torch_npu** | 华为 Ascend 的 PyTorch 适配库 | 项目中已有该依赖；FFT 接口完整 | 需处理部分算子兼容性问题         |
| **asnumpy**   | 类 NumPy 的 NPU 加速库        | API 接近 NumPy                 | 需源码编译；不支持 FFT；安装复杂 |

### 2. torch_npu 方案

#### 实现说明

实现思路与 CuPy 版本一致——将数据上传到 NPU 后在设备上完成计算，仅回传标量结果。关键差异在于 API 替换：

```python
import torch
import torch_npu  # 导入即注册 NPU 后端

# CuPy 版
g = cp.asarray(opt_gray, dtype=cp.float32)
dft = cp.fft.fft2(g)
shifted = cp.fft.fftshift(dft)
magnitude = cp.abs(shifted)

# torch_npu 版
g = torch.from_numpy(opt_gray).to(device=device, dtype=torch.float32)
dft = torch.fft.fft2(g)
shifted = torch.fft.fftshift(dft)
magnitude = torch.sqrt(shifted.real ** 2 + shifted.imag ** 2)
```

**兼容性问题与修复**：当前版本的 `torch_npu` 不支持对 `complex64` 类型的张量直接执行 `torch.abs`，会报错 `Tensor self not implemented for DT_COMPLEX64`。修复方式是手动计算复数的模：

```python
# 不可用：magnitude = torch.abs(shifted)
# 替代方案：
magnitude = torch.sqrt(shifted.real * shifted.real + shifted.imag * shifted.imag)
```

📄 **完整实现代码请参阅**：[npu_optimization_workbench/method_torch_npu.py](npu_optimization_workbench/method_torch_npu.py)

#### 测试环境

| 项目      | 配置                         |
| --------- | ---------------------------- |
| NPU       | Ascend 910B4                 |
| OS        | Ubuntu 22.04.5 LTS（Docker） |
| Python    | 3.11.13                      |
| torch     | 2.7.1                        |
| torch_npu | 2.7.1.post2                  |

> 注：测试期间 NPU 被其他项目占用，加速比可能略低于无竞争场景下的理论值。

#### 测试结果

测试条件：`assets` 目录前 15 张图像，每图重复 3 次，预热 2 次。

| 方法                  | 平均耗时       | FFT 平均       | 径向投影平均   | 加速比    |
| --------------------- | -------------- | -------------- | -------------- | --------- |
| numpy_ref（CPU 基线） | `423.20 ms`    | `357.80 ms`    | `63.96 ms`     | 1.00x     |
| **torch_npu (npu:0)** | **`99.30 ms`** | **`52.58 ms`** | **`45.54 ms`** | **4.26x** |

一致性验证：与 NumPy 参考实现的角度结果 MAE 为 `1.295e-06`，最大绝对误差为 `2.087e-06`，满足精度要求。

> 注：NPU 环境的 CPU 基线（`423.20 ms`）高于 GPU 环境的基线（`224.35 ms`），这是因为两台机器的 CPU 型号不同。加速比应在同一环境内对比。

### 3. asnumpy 方案

asnumpy 是一个面向 Ascend NPU 的类 NumPy 加速库，API 设计贴近 NumPy。

#### 安装

asnumpy 不提供预编译的 wheel 包，需要**从源码编译**安装：

```bash
# 1. 解压源码
unzip asnumpy-dtypes.zip && cd asnumpy-dtypes

# 2. 安装构建依赖
python3 -m pip install -r requirements.txt

# 3. 构建（需要下载 third_party 依赖：fmt、pybind11）
python3 -m build

# 4. 安装
python3 -m pip install dist/*.whl
```

构建过程中需要手动补齐 `third_party/fmt` 和 `third_party/pybind11` 的归档文件，且 pybind11 包需校验完整性（曾遇到压缩流 EOF 导致 CMake 找不到 `pybind11_add_module` 指令的问题）。CANN 8.0 版本环境下可正常完成编译。

#### 局限性

经检查，当前版本的 asnumpy **不支持 `fft2` / `fftshift` 接口**，因此无法直接替换去倾斜算法中占 68% 耗时的 FFT 链路。

对于其已实现的数学运算模块，单独做了基准测试：

📄 **测试脚本请参阅**：[npu_optimization_workbench/asnumpy_acceleration_benchmark.py](npu_optimization_workbench/asnumpy_acceleration_benchmark.py)

| 运算场景                | NumPy (CPU) | asnumpy (NPU) | 加速比     |
| ----------------------- | ----------- | ------------- | ---------- |
| `multiply + sum`        | `6.36 ms`   | `3.65 ms`     | 1.74x      |
| `sin + cos + add + sum` | `46.31 ms`  | `4.20 ms`     | **11.02x** |

> 测试矩阵尺寸：2048×2048，预热 2 次，重复 3 次。

在三角函数密集运算场景下 asnumpy 加速明显，但由于不支持 FFT，无法独立覆盖去倾斜算法全链路。

📄 **asnumpy 详细测试报告请参阅**：[npu_optimization_workbench/ASNUMPY_ACCELERATION_REPORT.md](npu_optimization_workbench/ASNUMPY_ACCELERATION_REPORT.md)

### 4. 混合方案实验：torch_npu FFT + asnumpy 径向投影

尝试将两者结合——FFT 阶段使用 torch_npu，径向投影中的三角函数部分使用 asnumpy，观察是否有叠加收益。

📄 **混合方案实现请参阅**：[npu_optimization_workbench/method_hybrid_torch_asnumpy.py](npu_optimization_workbench/method_hybrid_torch_asnumpy.py)

| 方法                          | 平均耗时        | FFT 平均    | 径向投影平均 | 加速比    |
| ----------------------------- | --------------- | ----------- | ------------ | --------- |
| numpy_ref（CPU 基线）         | `430.64 ms`     | `362.95 ms` | `66.37 ms`   | 1.00x     |
| **torch_npu**                 | **`111.59 ms`** | `64.20 ms`  | `46.17 ms`   | **3.86x** |
| hybrid（torch_npu + asnumpy） | `147.36 ms`     | `73.79 ms`  | `72.31 ms`   | 2.92x     |

混合方案的整体耗时（`147.36 ms`）反而**高于**纯 torch_npu 方案（`111.59 ms`）。原因是混合方案需要在 torch_npu 和 asnumpy 之间进行数据格式转换（torch tensor → numpy → asnumpy ndarray），这些转换的开销抵消了 asnumpy 在三角函数计算上的加速收益。

### 5. NPU 方案结论

**推荐使用纯 torch_npu 方案。** 理由：

1. 端到端加速效果最好（4.26x vs 混合方案的 2.92x）
2. 无额外编译安装负担
3. 角度精度与 NumPy 基线一致（MAE < 1e-6）

使用示例：

```python
from npu_optimization_workbench.method_torch_npu import get_angle

import cv2
image = cv2.imread("input.jpg")
angle = get_angle(image, amax=45.0, V=2048, W=304, D=0.55, device="npu:0")
print(f"倾斜角度: {angle:.4f}°")
```

命令行用法：

```bash
python3 npu_optimization_workbench/method_torch_npu.py input.jpg --device npu:0
```

📄 **NPU 适配完整报告请参阅**：[npu_optimization_workbench/NPU_ADAPTATION_REPORT.md](npu_optimization_workbench/NPU_ADAPTATION_REPORT.md)

---

## 五、总结

### 全链路加速效果一览

| 环境           | 方法             | 平均耗时       | 加速比     | 备注                  |
| -------------- | ---------------- | -------------- | ---------- | --------------------- |
| CPU            | original (NumPy) | `218~224 ms`   | 1.0x       | 基线                  |
| CPU            | cpu_numpy_vec    | `207~212 ms`   | 1.05x      | 向量化径向投影        |
| CPU            | cpu_numba_jit    | `180~183 ms`   | 1.19~1.25x | JIT 编译径向投影      |
| **NVIDIA GPU** | **CuPy**         | **`24.59 ms`** | **9.12x**  | RTX 3090              |
| **Ascend NPU** | **torch_npu**    | **`99.30 ms`** | **4.26x**  | 910B4，设备有竞争负载 |

### 推荐方案

- **NVIDIA GPU 环境**：使用 CuPy 方案，安装简单（`pip install cupy-cuda12x`），加速效果显著（9x）。
- **Ascend NPU 环境**：使用 torch_npu 方案，不引入额外依赖，加速效果可观（4x+）。
- **纯 CPU 环境**：使用 Numba JIT 方案，可获得约 1.2x 的加速，收益有限但无需 GPU/NPU。

📄 **所有实现的完整代码和基准数据均保存在以下目录中**：

- GPU 优化：[speed_optimization_workbench/](speed_optimization_workbench/)
- NPU 适配：[npu_optimization_workbench/](npu_optimization_workbench/)
- 性能分析：[for_eval_deskew/](for_eval_deskew/)

代码助手：Claude

写作助手：Claude

人类苦工：我
